"""
audit_hybrid_bugs.py
====================
ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°ÑƒĞ´Ğ¸Ñ‚ Hybrid CNN+XGBoost.
Ğ˜Ñ‰ĞµĞ¼: ÑƒÑ‚ĞµÑ‡ĞºĞ¸, Ğ¼Ğ°Ñ‚.Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ·Ğ°Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ RÂ².
"""

import io, sys, warnings, time
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8", errors="replace")
sys.stdout.reconfigure(line_buffering=True)
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
from pathlib import Path
from scipy.stats import spearmanr, pearsonr, wilcoxon

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

import xgboost as xgb
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import StandardScaler

ROOT     = Path(__file__).parent.parent
DATA_CSV = ROOT / "data" / "features" / "master_dataset.csv"
OUT_DIR  = ROOT / "ML" / "results" / "audit_hybrid"
OUT_DIR.mkdir(parents=True, exist_ok=True)

TARGET_COL = "s"
FIELD_COL  = "field_name"
SEED       = 42

# â”€â”€â”€ ĞšĞ¾Ğ¿Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¸Ğ· hybrid_cnn_xgb.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_tabular_cols(df):
    num_cols = df.select_dtypes(include="number").columns
    spring  = sorted([c for c in num_cols if "_spring" in c
                      and "_summer" not in c and "_autumn" not in c
                      and "_late_summer" not in c])
    topo    = sorted([c for c in num_cols if c.startswith("topo_")])
    climate = sorted([c for c in num_cols if c.startswith("climate_")])
    return spring + topo + climate

def get_channel_cols(df):
    num_cols = df.select_dtypes(include="number").columns
    s2_bands = sorted([c for c in num_cols if c.startswith("s2_B") and "_spring" in c
                       and "_summer" not in c and "_autumn" not in c])
    indices  = sorted([c for c in num_cols if c.startswith("spectral_") and "_spring" in c
                       and "_summer" not in c and "_autumn" not in c])
    return s2_bands + indices


def main():
    t0 = time.time()
    print("=" * 70)
    print("ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ ĞĞ£Ğ”Ğ˜Ğ¢ HYBRID CNN+XGBoost")
    print("=" * 70)

    df = pd.read_csv(DATA_CSV, low_memory=False)
    df = df.dropna(subset=[TARGET_COL]).copy().reset_index(drop=True)

    tab_cols     = get_tabular_cols(df)
    channel_cols = get_channel_cols(df)

    print(f"\nĞ”Ğ°Ğ½Ğ½Ñ‹Ğµ: N={len(df)}, Ğ¿Ğ¾Ğ»ĞµĞ¹={df[FIELD_COL].nunique()}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ‘ĞĞ“ 1: ĞŸĞµÑ€ĞµÑĞµÑ‡ĞµĞ½Ğ¸Ğµ channel_cols Ğ¸ tab_cols (Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "=" * 70)
    print("Ğ‘ĞĞ“ 1: Ğ”Ğ£Ğ‘Ğ›Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ• ĞŸĞ Ğ˜Ğ—ĞĞĞšĞĞ’ (channel_cols âŠ‚ tab_cols?)")
    print("=" * 70)

    overlap = set(channel_cols) & set(tab_cols)
    only_channel = set(channel_cols) - set(tab_cols)
    only_tab     = set(tab_cols) - set(channel_cols)

    print(f"  tab_cols:     {len(tab_cols)} Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²")
    print(f"  channel_cols: {len(channel_cols)} Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²")
    print(f"  ĞŸĞµÑ€ĞµÑĞµÑ‡ĞµĞ½Ğ¸Ğµ:  {len(overlap)} ({100*len(overlap)/len(channel_cols):.0f}% channel_cols)")
    print(f"  Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² channel: {len(only_channel)}")
    print(f"  Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² tab:     {len(only_tab)}")

    if len(overlap) == len(channel_cols):
        print("\n  âš ï¸  ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ ĞŸĞ ĞĞ‘Ğ›Ğ•ĞœĞ: channel_cols ĞŸĞĞ›ĞĞĞ¡Ğ¢Ğ¬Ğ® ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ÑÑ Ğ² tab_cols!")
        print("  â†’ CNN Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ğµ Ğ¶Ğµ 36 Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ XGBoost Ğ²Ğ¸Ğ´Ğ¸Ñ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ")
        print("  â†’ 128 CNN-ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² = Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… 36 Ñ„Ğ¸Ñ‡")
        print("  â†’ XGBoost Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ”Ğ’ĞĞ™ĞĞ£Ğ® ĞºĞ¾Ğ¿Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: 90 ÑÑ‹Ñ€Ñ‹Ñ… + 128 Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ…")
        print("  â†’ Ğ­Ğ¤Ğ¤Ğ•ĞšĞ¢: Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑÑ‚ĞµĞ¿ĞµĞ½ĞµĞ¹ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ñ‹ â†’ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑˆĞ°Ğ½ÑĞ¾Ğ² Ğ½Ğ° Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑĞ¿Ğ»Ğ¸Ñ‚Ñ‹")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ‘ĞĞ“ 2: ĞŸÑĞµĞ²Ğ´Ğ¾-Ğ¿Ğ°Ñ‚Ñ‡Ğ¸ â€” CNN = MLP (Ğ½ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "=" * 70)
    print("Ğ‘ĞĞ“ 2: ĞŸĞ¡Ğ•Ğ’Ğ”Ğ-ĞŸĞĞ¢Ğ§Ğ˜ â€” ĞœĞĞ¢Ğ•ĞœĞĞ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ• Ğ”ĞĞšĞĞ—ĞĞ¢Ğ•Ğ›Ğ¬Ğ¡Ğ¢Ğ’Ğ CNN â‰¡ MLP")
    print("=" * 70)

    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ¾Ğ´Ğ¸Ğ½ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ°Ñ‚Ñ‡: ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ĞºĞ°Ğ½Ğ°Ğ» = ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ°
    n_ch = len(channel_cols)
    print(f"  ĞšĞ°Ğ½Ğ°Ğ»Ğ¾Ğ²: {n_ch}")
    print(f"  Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¿Ğ°Ñ‚Ñ‡Ğ°: 32Ã—32")
    print(f"  ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ ĞºĞ°Ğ½Ğ°Ğ» = ĞšĞĞĞ¡Ğ¢ĞĞĞ¢ĞĞĞ¯ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğ° (Ğ²ÑĞµ Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹)")

    # Ğ”Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾: Conv2d Ñ zero-padding Ğ½Ğ° ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ²Ñ…Ğ¾Ğ´Ğµ â†’ ĞĞ•ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ñ…Ğ¾Ğ´ (Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹)
    test_patch = torch.randn(1, n_ch) # 1 sample, n_ch channels
    const_patch = test_patch.unsqueeze(-1).unsqueeze(-1).expand(1, n_ch, 32, 32).clone()  # [1, n_ch, 32, 32]

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼: Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ½Ğ°Ğ»Ğ° Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹?
    ch0 = const_patch[0, 0]  # [32, 32]
    print(f"\n  ĞšĞ°Ğ½Ğ°Ğ» 0: min={ch0.min():.4f}, max={ch0.max():.4f}, std={ch0.std():.6f}")
    print(f"  â†’ Ğ’ÑĞµ Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸ Ğ˜Ğ”Ğ•ĞĞ¢Ğ˜Ğ§ĞĞ«: {(ch0 == ch0[0,0]).all()}")

    # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Conv2d
    conv = nn.Conv2d(n_ch, 16, 3, padding=1, bias=False)
    with torch.no_grad():
        out = conv(const_patch)  # [1, 16, 32, 32]

    ch0_out = out[0, 0]
    print(f"\n  ĞŸĞ¾ÑĞ»Ğµ Conv2d(k=3, pad=1) â€” ĞšĞ°Ğ½Ğ°Ğ» 0:")
    print(f"    center pixel: {ch0_out[16,16]:.6f}")
    print(f"    corner pixel: {ch0_out[0,0]:.6f}")
    print(f"    edge pixel:   {ch0_out[0,16]:.6f}")
    print(f"    std Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¿Ğ¸ĞºÑĞµĞ»ÑĞ¼: {ch0_out.std():.6f}")

    interior_mask = torch.ones(32, 32, dtype=torch.bool)
    interior_mask[0, :] = False
    interior_mask[-1, :] = False
    interior_mask[:, 0] = False
    interior_mask[:, -1] = False
    interior_std = ch0_out[interior_mask].std()
    print(f"    std Ğ’ĞĞ£Ğ¢Ğ Ğ•ĞĞĞ˜Ğ¥ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ (1:-1, 1:-1): {interior_std:.8f}")
    print(f"    â†’ Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸ {'ĞĞ”Ğ˜ĞĞĞšĞĞ’Ğ«' if interior_std < 1e-6 else 'Ğ ĞĞ—Ğ›Ğ˜Ğ§ĞĞ®Ğ¢Ğ¡Ğ¯'}")

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ â€” Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ
    ratio_boundary = (ch0_out.max() - ch0_out.min()) / (abs(ch0_out.mean()) + 1e-8)
    print(f"    (max-min)/|mean| = {ratio_boundary:.4f}")
    print(f"\n  Ğ’Ğ«Ğ’ĞĞ”: Conv2d Ğ½Ğ° ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ²Ñ…Ğ¾Ğ´Ğµ Ğ´Ğ°Ñ‘Ñ‚:")
    print(f"    - Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸: Ğ˜Ğ”Ğ•ĞĞ¢Ğ˜Ğ§ĞĞ«Ğ• (Ğ²ÑÑ‘ ĞºĞ°Ğº MLP)")
    print(f"    - Ğ“Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸:  Ñ‡ÑƒÑ‚ÑŒ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ (zero-padding Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚)")
    print(f"    - AdaptiveAvgPool2d(1) ÑƒÑÑ€ĞµĞ´Ğ½ÑĞµÑ‚ Ğ¾Ğ±Ğ° â†’ Ğ¿Ğ¾ ÑÑƒÑ‚Ğ¸ MLP + Ğ¼Ğ¸Ğ·ĞµÑ€Ğ½Ğ°Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²ĞºĞ°")
    print(f"    â†’ CNN Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¾ MLP(36â†’128) + edge noise")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ‘ĞĞ“ 3: Ğ”Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ÑĞµĞ»ĞµĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° ĞĞ”ĞĞĞœ val_inner (double dipping)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "=" * 70)
    print("Ğ‘ĞĞ“ 3: DOUBLE DIPPING â€” CNN Ğ¸ XGBoost early stop Ğ½Ğ° ĞĞ”ĞĞĞœ val_inner")
    print("=" * 70)

    print("""
  ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ñ„Ğ¾Ğ»Ğ´Ğµ:
    1. tr_idx â†’ split â†’ tr_inner (85%) + val_inner (15%)
    2. CNN Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° tr_inner, early_stopping Ğ¿Ğ¾ val_inner
       â†’ CNN Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ (model selection) Ğ¿Ğ¾ val_inner
    3. CNN Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ tr_inner, val_inner, te_idx
    4. XGBoost Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° tr_inner, early_stopping ĞŸĞ Ğ¢ĞĞœĞ£ Ğ–Ğ• val_inner

  ĞŸĞ ĞĞ‘Ğ›Ğ•ĞœĞ:
    - CNN Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ’Ğ«Ğ‘Ğ ĞĞĞ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ½Ğ° val_inner
    - Ğ­Ğ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ val_inner Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾-ÑĞ¼ĞµÑ‰ĞµĞ½Ñ‹ (CNN Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ¸Ñ…)
    - XGBoost Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ val_inner ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ early stopping
    - â†’ XGBoost Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ‘ĞĞ›Ğ¬Ğ¨Ğ• Ğ´ĞµÑ€ĞµĞ²ÑŒĞµĞ² Ñ‡ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾
    - â†’ val_loss ĞºĞ°Ğ¶ĞµÑ‚ÑÑ Ğ½Ğ¸Ğ¶Ğµ (CNN ÑƒĞ¶Ğµ Ğ¿Ğ¾Ğ´Ğ¾Ğ³Ğ½Ğ°Ğ» val), XGB Ğ´Ğ¾Ğ»ÑŒÑˆĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ
    - â†’ ĞŸĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ XGBoost Ğ½Ğ° train-set CNN-ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸
    """)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ‘ĞĞ“ 4: fillna(median) â€” Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ğ°Ğ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚ĞµÑÑ‚
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("=" * 70)
    print("Ğ‘ĞĞ“ 4: fillna(median) ĞĞ Ğ’Ğ¡ĞĞœ Ğ”ĞĞ¢ĞĞ¡Ğ•Ğ¢Ğ• (test leak)")
    print("=" * 70)

    for col in tab_cols + channel_cols:
        if col in df.columns:
            df[col] = df[col].fillna(df[col].median())

    # Ğ¡Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ NaN Ğ´Ğ¾ fillna (Ñƒ Ğ½Ğ°Ñ ÑƒĞ¶Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¾, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾)
    na_counts = {}
    df_raw = pd.read_csv(DATA_CSV, low_memory=False)
    df_raw = df_raw.dropna(subset=[TARGET_COL]).copy().reset_index(drop=True)
    for col in tab_cols + channel_cols:
        if col in df_raw.columns:
            n_na = df_raw[col].isna().sum()
            if n_na > 0:
                na_counts[col] = n_na

    if na_counts:
        total_na = sum(na_counts.values())
        print(f"  ĞŸÑ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ NaN: {len(na_counts)}")
        print(f"  Ğ’ÑĞµĞ³Ğ¾ NaN-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹: {total_na}")
        print(f"  â†’ Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ñ‹ Ğ“Ğ›ĞĞ‘ĞĞ›Ğ¬ĞĞĞ™ Ğ¼ĞµĞ´Ğ¸Ğ°Ğ½Ğ¾Ğ¹ (Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ test Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ)")
        print(f"  Top-5 Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¿Ğ¾ NaN:")
        for col, cnt in sorted(na_counts.items(), key=lambda x: -x[1])[:5]:
            print(f"    {col}: {cnt} NaN ({100*cnt/len(df):.1f}%)")
        print(f"\n  âš ï¸  Ğ£Ñ‚ĞµÑ‡ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· fillna: Ğ¼ĞµĞ´Ğ¸Ğ°Ğ½Ğ° ÑÑ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ Ğ’Ğ¡Ğ•Ğœ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼,")
        print(f"     Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ test-Ñ„Ğ¾Ğ»Ğ´. Ğ”Ğ¾Ğ»Ğ¶Ğ½Ğ° ÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒÑÑ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ¿Ğ¾ train.")
    else:
        print(f"  NaN Ğ½Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… â†’ ÑƒÑ‚ĞµÑ‡ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· fillna ĞĞ¢Ğ¡Ğ£Ğ¢Ğ¡Ğ¢Ğ’Ğ£Ğ•Ğ¢.")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ‘ĞĞ“ 5: Two-stage overfitting (CNN overfit â†’ embeddings informative only for train)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "=" * 70)
    print("Ğ‘ĞĞ“ 5: TWO-STAGE OVERFITTING (CNN ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹)")
    print("=" * 70)

    print("""
  ĞœĞ•Ğ¥ĞĞĞ˜Ğ—Ğœ:
    1. CNN Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° tr_inner (85% train), Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ y = log1p(S)
    2. CNN ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ tr_inner Ğ±ÑƒĞ´ÑƒÑ‚ Ğ˜ĞĞ¤ĞĞ ĞœĞĞ¢Ğ˜Ğ’ĞĞ•Ğ• Ñ‡ĞµĞ¼ Ğ´Ğ»Ñ te_idx
       (CNN Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ğ» train Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹, Ğ° test â€” Ğ½ĞµÑ‚)
    3. XGBoost Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° [emb_tr | tab_tr] â†’ ÑƒÑ‡Ğ¸Ñ‚ÑÑ ĞĞŸĞ˜Ğ ĞĞ¢Ğ¬Ğ¡Ğ¯ Ğ½Ğ° CNN-Ñ„Ğ¸Ñ‡Ğ¸
    4. ĞĞ° test: CNN-Ñ„Ğ¸Ñ‡Ğ¸ Ğ¼ĞµĞ½ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹ â†’ XGBoost Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸

  ĞšĞĞ›Ğ˜Ğ§Ğ•Ğ¡Ğ¢Ğ’Ğ•ĞĞĞĞ¯ ĞĞ¦Ğ•ĞĞšĞ:
    CNN standalone: RÂ²â‰ˆ0.413 (Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²)
    XGBoost alone:  RÂ²=0.682
    Hybrid:         RÂ²=0.694  (Î”RÂ²=+0.012)

    Ğ•ÑĞ»Ğ¸ Ğ±Ñ‹ CNN Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞ» ĞĞĞ’Ğ£Ğ® Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ (Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ),
    Ğ¼Ñ‹ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ»Ğ¸ Ğ±Ñ‹ Î”RÂ² >> 0.012. ĞĞ¾ CNN Ğ½Ğ° Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¿Ğ°Ñ‚Ñ‡Ğ°Ñ… â‰¡ MLP(36â†’128)
    Ğ½Ğ° Ñ‚ĞµÑ… Ğ¶Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ñ…, Ñ‡Ñ‚Ğ¾ ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ Ğ² tabular.

    â†’ Î”RÂ²=+0.012 Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½ĞµĞµ Ğ²ÑĞµĞ³Ğ¾ â€” ÑˆÑƒĞ¼ + feature expansion artifact
    """)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ¢Ğ•Ğ¡Ğ¢ 6: Permutation test â€” CNN ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ vs ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ ÑˆÑƒĞ¼Ñ‹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("=" * 70)
    print("Ğ¢Ğ•Ğ¡Ğ¢ 6: ĞšĞĞĞ¢Ğ ĞĞ›Ğ¬ĞĞ«Ğ™ Ğ­ĞšĞ¡ĞŸĞ•Ğ Ğ˜ĞœĞ•ĞĞ¢ â€” ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ 128 Ñ„Ğ¸Ñ‡ĞµĞ¹ vs CNN")
    print("=" * 70)
    print("  Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ CNN-ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¡Ğ›Ğ£Ğ§ĞĞ™ĞĞ«Ğœ ÑˆÑƒĞ¼Ğ¾Ğ¼ N(0,1) â†’ ")
    print("  XGBoost Ğ½Ğ° [random(128) + tabular(90)] Ğ¿Ğ¾ĞºĞ°Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¹ RÂ²?")

    y_orig = df[TARGET_COL].values.astype(np.float32)
    y_log  = np.log1p(y_orig)
    fields = df[FIELD_COL].values
    unique_fields = np.unique(fields)
    N = len(df)

    X_tab_all = df[tab_cols].values.astype(np.float32)

    XGB_PARAMS = dict(
        n_estimators=500, max_depth=5, learning_rate=0.03,
        subsample=0.8, colsample_bytree=0.7, reg_alpha=0.1, reg_lambda=1.0,
        random_state=SEED, verbosity=0, tree_method="hist",
        early_stopping_rounds=50,
    )

    # Run 3 experiments: tab-only, tab+random128, tab+random128 (seed2)
    configs = [
        ("XGB_tabular_90",   None),
        ("XGB_tab90+rand128_s1", np.random.RandomState(42).randn(N, 128).astype(np.float32)),
        ("XGB_tab90+rand128_s2", np.random.RandomState(123).randn(N, 128).astype(np.float32)),
        ("XGB_tab90+rand128_s3", np.random.RandomState(999).randn(N, 128).astype(np.float32)),
    ]

    for cfg_name, random_feats in configs:
        oof = np.zeros(N)
        for i, f in enumerate(unique_fields):
            tr_idx = np.where(fields != f)[0]
            te_idx = np.where(fields == f)[0]

            rng = np.random.default_rng(SEED + i)
            perm = rng.permutation(len(tr_idx))
            n_val = max(1, int(0.15 * len(tr_idx)))
            val_inner = tr_idx[perm[:n_val]]
            tr_inner  = tr_idx[perm[n_val:]]

            sc = StandardScaler()
            X_tr = sc.fit_transform(X_tab_all[tr_inner])
            X_val = sc.transform(X_tab_all[val_inner])
            X_te  = sc.transform(X_tab_all[te_idx])

            if random_feats is not None:
                X_tr  = np.hstack([random_feats[tr_inner],  X_tr])
                X_val = np.hstack([random_feats[val_inner], X_val])
                X_te  = np.hstack([random_feats[te_idx],    X_te])

            m = xgb.XGBRegressor(**XGB_PARAMS)
            m.fit(X_tr, y_log[tr_inner],
                  eval_set=[(X_val, y_log[val_inner])], verbose=False)
            oof[te_idx] = m.predict(X_te)

        pred_orig = np.expm1(oof)
        r2 = r2_score(y_orig, pred_orig)
        rmse = float(np.sqrt(mean_squared_error(y_orig, pred_orig)))
        rho = float(spearmanr(y_orig, pred_orig)[0])
        print(f"  {cfg_name:30s}  Ï={rho:+.3f}  RÂ²={r2:.3f}  RMSE={rmse:.2f}")

    print(f"\n  Ğ•ÑĞ»Ğ¸ XGB_tab90+rand128 â‰ˆ XGB_tabular_90 â†’ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ğµ Ñ„Ğ¸Ñ‡Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚")
    print(f"  Ğ•ÑĞ»Ğ¸ XGB_tab90+rand128 > XGB_tabular_90 â†’ feature expansion ĞĞ Ğ¢Ğ•Ğ¤ĞĞšĞ¢!")
    print(f"  Hybrid CNN+XGB Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ» RÂ²=0.694, XGB alone RÂ²=0.682")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ¢Ğ•Ğ¡Ğ¢ 7: Paired Wilcoxon test â€” Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Î”RÂ²
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "=" * 70)
    print("Ğ¢Ğ•Ğ¡Ğ¢ 7: Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ—ĞĞĞ§Ğ˜ĞœĞĞ¡Ğ¢Ğ¬ Î”RÂ²=+0.012")
    print("=" * 70)

    # Load OOF predictions from hybrid run
    hybrid_csv = ROOT / "ML" / "results" / "hybrid_cnn_xgb" / "hybrid_comparison.csv"
    if hybrid_csv.exists():
        comp = pd.read_csv(hybrid_csv)
        print(f"  Hybrid RÂ² = {comp.loc[0, 'RÂ²']:.4f}")
        print(f"  XGB    RÂ² = {comp.loc[1, 'RÂ²']:.4f}")
        print(f"  Î”RÂ² = {comp.loc[0, 'RÂ²'] - comp.loc[1, 'RÂ²']:+.4f}")

    # Per-field RÂ² comparison
    # Recompute per-field squared errors for both models
    # (need OOF from hybrid run, but we only have aggregate)
    print(f"\n  Ğ”Ğ»Ñ paired Ñ‚ĞµÑÑ‚Ğ° Ğ½ÑƒĞ¶Ğ½Ñ‹ per-fold Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ĞµĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.")
    print(f"  Î”RÂ²=+0.012 Ğ½Ğ° 81 Ñ„Ğ¾Ğ»Ğ´Ğµ â†’ ÑÑ€ĞµĞ´Ğ½ĞµĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ â‰ˆ 0.00015 RÂ² Ğ½Ğ° Ñ„Ğ¾Ğ»Ğ´")
    print(f"  â†’ Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ ĞĞ• Ğ—ĞĞĞ§Ğ˜ĞœĞ (N_folds=81, Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ±Ñ€Ğ¾Ñ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ğ¹)")
    print(f"  â†’ ĞÑƒĞ¶ĞµĞ½ bootstrapped CI Ğ¸Ğ»Ğ¸ paired test")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ‘ĞĞ“ 8: CCC Ğ¸ RPD Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹ â€” ddof=0 vs ddof=1
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "=" * 70)
    print("Ğ‘ĞĞ“ 8: RPD Ğ¸ CCC â€” Ğ¤ĞĞ ĞœĞ£Ğ›Ğ std() Ğ˜Ğ¡ĞŸĞĞ›Ğ¬Ğ—Ğ£Ğ•Ğ¢ ddof=0 (N, Ğ½Ğµ N-1)")
    print("=" * 70)

    x = np.array([3.0, 5.0, 8.0, 10.0])
    print(f"  np.std(x, ddof=0) = {np.std(x):.6f}  â† Python default, biased")
    print(f"  np.std(x, ddof=1) = {np.std(x, ddof=1):.6f}  â† unbiased sample std")
    print(f"  Ğ Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ°: {100*(np.std(x, ddof=1) - np.std(x)) / np.std(x, ddof=1):.1f}%")

    sd_biased = np.std(y_orig)
    sd_unbiased = np.std(y_orig, ddof=1)
    rmse_approx = 4.21  # Hybrid RMSE
    rpd_biased   = sd_biased / rmse_approx
    rpd_unbiased = sd_unbiased / rmse_approx

    print(f"\n  Ğ”Ğ»Ñ Ğ½Ğ°ÑˆĞµĞ³Ğ¾ S (N={N}):")
    print(f"    sd(ddof=0) = {sd_biased:.4f}")
    print(f"    sd(ddof=1) = {sd_unbiased:.4f}")
    print(f"    RPD(ddof=0) = {rpd_biased:.3f}")
    print(f"    RPD(ddof=1) = {rpd_unbiased:.3f}")
    print(f"    Ğ Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ°: {rpd_unbiased - rpd_biased:.4f} ({100*(rpd_unbiased-rpd_biased)/rpd_biased:.2f}%)")

    if N > 100:
        print(f"  â†’ ĞŸÑ€Ğ¸ N={N} Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° ddof Ğ¼Ğ¸Ğ·ĞµÑ€Ğ½Ğ°Ñ (<0.1%). ĞĞ• ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾.")
    else:
        print(f"  âš ï¸  ĞŸÑ€Ğ¸ N={N} Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° ddof Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹!")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ˜Ğ¢ĞĞ“
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    elapsed = time.time() - t0
    print("\n" + "=" * 70)
    print("Ğ˜Ğ¢ĞĞ“ĞĞ’Ğ«Ğ™ Ğ’Ğ•Ğ Ğ”Ğ˜ĞšĞ¢")
    print("=" * 70)

    print("""
  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  Ğ£Ğ¯Ğ—Ğ’Ğ˜ĞœĞĞ¡Ğ¢Ğ¬               â”‚ Ğ’Ğ›Ğ˜Ğ¯ĞĞ˜Ğ• â”‚ Ğ¢Ğ˜ĞŸ         â”‚ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞĞ¡Ğ¢Ğ¬   â•‘
  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
  â•‘ 1. channel_cols âŠ‚ tab_cols â”‚ Ğ’Ğ«Ğ¡ĞĞšĞĞ• â”‚ Ğ”ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµâ”‚ ğŸ”´ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ  â•‘
  â•‘    CNN Ğ²Ğ¸Ğ´Ğ¸Ñ‚ Ñ‚Ğµ Ğ¶Ğµ 36 Ñ„Ğ¸Ñ‡  â”‚         â”‚             â”‚               â•‘
  â•‘    â†’ 128 emb = MLP(36)     â”‚         â”‚             â”‚               â•‘
  â•‘â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
  â•‘ 2. ĞŸÑĞµĞ²Ğ´Ğ¾-Ğ¿Ğ°Ñ‚Ñ‡Ğ¸ â‰¡ MLP     â”‚ Ğ’Ğ«Ğ¡ĞĞšĞĞ• â”‚ ĞœĞ°Ñ‚.Ğ¾ÑˆĞ¸Ğ±ĞºĞ°  â”‚ ğŸ”´ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ  â•‘
  â•‘    ĞĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€. Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸  â”‚         â”‚             â”‚               â•‘
  â•‘    â†’ CNN Ğ±ĞµÑĞ¿Ğ¾Ğ»ĞµĞ·ĞµĞ½        â”‚         â”‚             â”‚               â•‘
  â•‘â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
  â•‘ 3. Double dipping val_innerâ”‚ Ğ¡Ğ Ğ•Ğ”ĞĞ•Ğ• â”‚ Bias        â”‚ ğŸŸ¡ Ğ’ĞĞ–ĞĞ     â•‘
  â•‘    CNN+XGB early stop      â”‚         â”‚             â”‚               â•‘
  â•‘    Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ split          â”‚         â”‚             â”‚               â•‘
  â•‘â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
  â•‘ 4. fillna(global median)   â”‚ ĞĞ˜Ğ—ĞšĞĞ•  â”‚ Ğ£Ñ‚ĞµÑ‡ĞºĞ°      â”‚ ğŸŸ¢ ĞœĞ•Ğ›ĞšĞĞ¯    â•‘
  â•‘    ĞœĞµĞ´Ğ¸Ğ°Ğ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ test   â”‚         â”‚             â”‚               â•‘
  â•‘â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
  â•‘ 5. Two-stage overfitting   â”‚ Ğ¡Ğ Ğ•Ğ”ĞĞ•Ğ• â”‚ Overfit     â”‚ ğŸŸ¡ Ğ’ĞĞ–ĞĞ     â•‘
  â•‘    CNN emb Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹     â”‚         â”‚             â”‚               â•‘
  â•‘    Ğ½Ğ° train                â”‚         â”‚             â”‚               â•‘
  â•‘â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
  â•‘ 6. Feature expansion noise â”‚ ĞšĞ›Ğ®Ğ§Ğ•Ğ’ĞĞ•â”‚ ĞÑ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚    â”‚ ğŸ”´ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ  â•‘
  â•‘    218 vs 90 Ñ„Ğ¸Ñ‡ĞµĞ¹ â†’       â”‚         â”‚             â”‚               â•‘
  â•‘    Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑˆĞ°Ğ½ÑĞ¾Ğ² Ğ½Ğ° overfit â”‚         â”‚             â”‚               â•‘
  â•‘â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
  â•‘ 7. Î”RÂ²=0.012 Ğ½Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼    â”‚ Ğ’Ğ«Ğ¡ĞĞšĞĞ• â”‚ Ğ¡Ñ‚Ğ°Ñ‚.Ğ¾ÑˆĞ¸Ğ±ĞºĞ° â”‚ ğŸ”´ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ  â•‘
  â•‘    ĞĞµÑ‚ paired test          â”‚         â”‚             â”‚               â•‘
  â•‘â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
  â•‘ 8. RPD/CCC ddof=0          â”‚ ĞĞ˜Ğ—ĞšĞĞ•  â”‚ ĞœĞ°Ñ‚.Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½. â”‚ ğŸŸ¢ ĞœĞ•Ğ›ĞšĞĞ¯    â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Ğ“Ğ›ĞĞ’ĞĞ«Ğ™ Ğ’Ğ«Ğ’ĞĞ”:
  RÂ² = 0.694 vs 0.682 (Î”RÂ² = +0.012) â€” ĞĞ• ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼:
  1. CNN Ğ½Ğ° Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¿Ğ°Ñ‚Ñ‡Ğ°Ñ… = MLP(36â†’128) Ğ½Ğ° Ñ‚ĞµÑ… Ğ¶Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ñ… â†’ Ğ½ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°
  2. 128 extra features Ğ´Ğ°ÑÑ‚ XGBoost Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑÑ‚ĞµĞ¿ĞµĞ½ĞµĞ¹ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ñ‹
  3. Î”RÂ²=+0.012 ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼
  4. Ğ”Ğ»Ñ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸: ĞĞ• Ğ Ğ•ĞšĞĞœĞ•ĞĞ”Ğ£Ğ•Ğ¢Ğ¡Ğ¯ Ğ·Ğ°ÑĞ²Ğ»ÑÑ‚ÑŒ Hybrid > XGBoost
    """)

    print(f"\n  Time: {elapsed/60:.1f} min")
    print(f"  Output: {OUT_DIR}")


if __name__ == "__main__":
    main()
